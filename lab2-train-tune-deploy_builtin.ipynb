{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d60139-abfa-44d3-aa33-54cac4b3dd30",
   "metadata": {},
   "source": [
    "## Train, Tune and Deploy XGBoost (prebuilt containers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d25632-596b-4aec-abb5-9f0611599ea2",
   "metadata": {},
   "source": [
    "Here we will <b>train, tune, and deploy a model on Amazon SageMaker using the popular XGBoost ML algorithm with SageMaker's prebuilt containers</b>. SageMaker manages all of the underlying infrastructure to train your model at petabyte scale, and deploy it to production.\n",
    "\n",
    "In this tutorial, you will assume the role of a machine learning developer working at a bank. You have been asked to develop a machine learning model to predict whether a customer will enroll for a certificate of deposit (CD). The model will be trained on the marketing dataset that contains information on customer demographics, responses to marketing events, and external factors.\n",
    "\n",
    "The data has been labeled for your convenience and a column in the dataset identifies whether the customer is enrolled for a product offered by the bank. A version of this dataset is publicly available  from the ML repository curated by the University of California, Irvine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfd272-0663-441d-b2bb-e017591fa5d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Train and tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3e34f-1d82-4f96-9c7a-cb01b1164d5c",
   "metadata": {},
   "source": [
    "In this step, we will train a machine learning model with the training dataset which was uploaded in an Amazon S3 bucket in the previous Lab (SageMaker Processing). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e93d8454-14a6-4ee3-b11b-419e6d08e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Default bucket: sagemaker-eu-central-1-365644463685\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# cell 01: Setup the environment\n",
    "import sagemaker\n",
    "bucket = sagemaker.Session().default_bucket()  # (re)define default bucket\n",
    "print('Default bucket:', bucket)\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# (Re)define train - test - validation paths:\n",
    "train_path = f\"s3://{bucket}/{prefix}/train\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30ef17-c11b-4a8e-bb56-56803453172f",
   "metadata": {},
   "source": [
    "First, we will <b>specify XGBoost ECR container location</b> for SageMaker's implementation of XGBoost.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efb8702d-42bd-4af0-a832-63fd9ffc2552",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "# cell 10: Specify location of XGBoost ECR container\n",
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff4233-3869-4a06-9f43-d28fb0f15519",
   "metadata": {},
   "source": [
    "Then, we specify <b>train and validation data set location</b>. Because we're training with the CSV file format, we'll create `s3_input` so that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d22d6984-eeb5-41ac-9310-f91b02358c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 11: Specify train and validation set location\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_path.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=validation_path.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e3f1f-1da5-4265-adb6-503cb8248d08",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, <b>define the training parameters in SageMaker Estimator</b>. This includes:\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data\n",
    "1. Algorithm hyperparameters\n",
    "\n",
    "And then a `.fit()` function which specifies:\n",
    "1. <b>S3 location for output data</b>.  In this case we have both a training and validation set which are passed in.\n",
    "\n",
    "You will also define the tuning hyperparameters in set_hyperparameters and then call the “fit” method to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7a52ed3-d7c2-40c0-9b89-5726733929cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgboost-2023-11-16-14-32-10-312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-16 14:32:10 Starting - Starting the training job...\n",
      "2023-11-16 14:32:26 Starting - Preparing the instances for training......\n",
      "2023-11-16 14:33:36 Downloading - Downloading input data...\n",
      "2023-11-16 14:34:06 Training - Downloading the training image......\n",
      "2023-11-16 14:35:07 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2023-11-16:14:34:57:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2023-11-16:14:34:57:INFO] File size need to be processed in the node: 4.35mb. Available memory size in the node: 8561.76mb\u001b[0m\n",
      "\u001b[34m[2023-11-16:14:34:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[14:34:57] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[14:34:57] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2023-11-16:14:34:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[14:34:57] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[14:34:57] 8238x59 matrix with 486042 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.101384#011validation-error:0.102209\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.101037#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.100205#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.100309#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.101141#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.100447#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.100864#011validation-error:0.100024\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.10076#011validation-error:0.100267\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.100482#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.100621#011validation-error:0.100267\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.100309#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.100066#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.099997#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.099997#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.099754#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.099719#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.099754#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.099684#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.099823#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.099858#011validation-error:0.101602\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.099927#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.099546#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.099268#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.09906#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.098887#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.098921#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.099164#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.098991#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.099025#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.098783#011validation-error:0.100024\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.098644#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.098609#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 28 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.098262#011validation-error:0.099903\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.098297#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.098332#011validation-error:0.099903\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.098332#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.098019#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.098019#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.098019#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.098054#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.097985#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.098193#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.098089#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.098158#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.098262#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.098297#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.098193#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 20 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.098124#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 22 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.09795#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 34 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.098089#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.098124#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.097777#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.097569#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.097638#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.097777#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[14:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.097569#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.097673#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.097569#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 26 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.097465#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.097395#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.097395#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.097256#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.097256#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.097256#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 32 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.097187#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.09736#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.097291#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.09736#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 34 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.097326#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.097291#011validation-error:0.101602\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.097152#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.097083#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 24 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.097152#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.097014#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.097083#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 28 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.097048#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.09691#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 24 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.096771#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.096632#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.096806#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 30 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.096667#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 38 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.096632#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.096632#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.096632#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.096632#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.096736#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.096701#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.096597#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 22 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.096528#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.096493#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.096493#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.096493#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.096424#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[14:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 32 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.096355#011validation-error:0.100995\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-11-16 14:35:23 Completed - Training job completed\n",
      "Training seconds: 107\n",
      "Billable seconds: 107\n"
     ]
    }
   ],
   "source": [
    "# cell 12: define the training parameters in SageMaker Estimator object\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',  #'ml.c5.2xlarge',  #default: 'ml.m4.xlarge' \n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    #keep_alive_period_in_seconds=60*60,\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05561f6c-ea88-4273-b1ca-8a50b9d32684",
   "metadata": {},
   "source": [
    "You can also <b>check from AWS console to verify the training job started</b> and wait until the status becomes “Completed” (by going to SageMaker -> Training jobs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4a2900-6993-4c83-b4c9-b9e0c4e335d8",
   "metadata": {},
   "source": [
    "### 2. Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889751d-a600-4e70-8276-7384ec284c42",
   "metadata": {},
   "source": [
    "In this step, we deploy the trained model to a real-time HTTPS endpoint. This process can take around 6-8 min. You can also choose our newer instance type such as “ml.m5.xlarge” for this deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05035874-37ba-4b9a-9675-161bf082bffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: xgboost-2023-11-16-13-51-16-536\n",
      "INFO:sagemaker:Creating endpoint-config with name xgboost-2023-11-16-13-51-16-536\n",
      "INFO:sagemaker:Creating endpoint with name xgboost-2023-11-16-13-51-16-536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---!"
     ]
    }
   ],
   "source": [
    "# cell 13: Deploy trained model to real-time endpoint (with pre-built container)\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.c5.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe335142-e013-43f3-ae4e-3a424260fb18",
   "metadata": {},
   "source": [
    "Now we will <b>check the SageMaker endpoint deployment in the AWS console</b> view as well. In the SageMaker AWS Console, go to Inference -> \"Endpoints\" on the left pane. You will see the endpoint in “Creating” state, which will soon transition to “InService” state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097ccd5-eaf8-4844-aa7b-a3f629577a91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Predict and Evaluate model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16ac5f-1463-49a6-8734-24cdb431e5bd",
   "metadata": {},
   "source": [
    "In this step you will <b>reformat the CSV data, then run the model to create predictions</b>. There are many ways to compare the performance of a machine learning model, but let's start by simply comparing actual to predicted values (test set accuracy). In this case, we're simply predicting whether the customer subscribed to a term deposit (1) or not (0), which produces a simple confusion matrix that will help us evaluate model performance.\n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint. Our data is currently stored as NumPy arrays in memory of our notebook instance. To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV.\n",
    "Note: For inference with CSV format, SageMaker XGBoost requires that the data does NOT include the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cdd94eb-bc9a-4c95-9570-e53ad315ce7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 14: Create csv serializer object\n",
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cc7427-9960-403b-9ade-073560146271",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-central-1-365644463685/sagemaker/DEMO-xgboost-dm/test/test_x.csv to ../../../../tmp/test_x.csv\n",
      "download: s3://sagemaker-eu-central-1-365644463685/sagemaker/DEMO-xgboost-dm/test/test_y.csv to ../../../../tmp/test_y.csv\n"
     ]
    }
   ],
   "source": [
    "# # cell 15:\n",
    "!aws s3 cp $test_path/test_x.csv /tmp/test_x.csv\n",
    "!aws s3 cp $test_path/test_y.csv /tmp/test_y.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c8a49-6e31-4da1-91bd-fea88116f3f9",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869c9c2e-da6d-4765-b595-a1a1f8d9fade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 16: Produce test set predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def predict(data, predictor, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "test_x = pd.read_csv('/tmp/test_x.csv', names=[f'{i}' for i in range(59)])\n",
    "test_y = pd.read_csv('/tmp/test_y.csv', names=['y'])\n",
    "predictions = predict(test_x.drop(test_x.columns[0], axis=1).to_numpy(), xgb_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbf49f-7d7e-482a-9643-81ef4c63158b",
   "metadata": {},
   "source": [
    "Now we'll check our confusion matrix to see how well we predicted versus actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbafbcbe-1bb3-434b-82a7-8d6536558560",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3632</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>479</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions   0.0  1.0\n",
       "actuals               \n",
       "0            3632    3\n",
       "1             479    5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 17: Produce confusion matrix\n",
    "pd.crosstab(index=test_y['y'].values, columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878e3fa-ca38-4ff5-b838-2eb90237a9eb",
   "metadata": {},
   "source": [
    "So, of the ~4000 potential customers, we predicted 136 would subscribe and 94 of them actually did.  We also had 389 subscribers who subscribed that we did not predict would.  This is less than desirable, but the model can (and should) be tuned to improve this.  Most importantly, note that with minimal effort, our model produced accuracies similar to those published [here](http://media.salford-systems.com/video/tutorial/2015/targeted_marketing.pdf).\n",
    "\n",
    "_Note that because there is some element of randomness in the algorithm's subsample, your results may differ slightly from the text written above._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61f272-f91d-43a3-bfc8-236cc1932091",
   "metadata": {},
   "source": [
    "### 3. Automatic model-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94249cd-d4d7-40c7-b20b-94c7ce4d0818",
   "metadata": {},
   "source": [
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose.\n",
    "\n",
    "For example, suppose that you want to solve a binary classification  problem on this marketing dataset. Your goal is to maximize the area under the curve (auc)  metric of the algorithm by training an XGBoost Algorithm model. You don't know which values of the eta, alpha, min_child_weight, and max_depth hyperparameters to use to train the best model. To find the best values for these hyperparameters, you can specify ranges of values that Amazon SageMaker hyperparameter tuning searches to find the combination of values that results in the training job that performs the best as measured by the objective metric that you chose. Hyperparameter tuning launches training jobs that use hyperparameter values in the ranges that you specified, and returns the training job with highest auc.\n",
    "\n",
    "<b>We will tune four hyperparameters in this example:<b>\n",
    "\n",
    "eta: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative.\n",
    "\n",
    "min_child_weight: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is.\n",
    "\n",
    "alpha: L1 regularization term on weights. Increasing this value makes models more conservative.\n",
    "\n",
    "max_depth: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f1bed9-6a4d-4cb9-8b80-833f1f70a054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 18: Set search ranges for hyperparameters\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                            'min_child_weight': ContinuousParameter(1, 10),\n",
    "                            'alpha': ContinuousParameter(0, 2),\n",
    "                            'max_depth': IntegerParameter(1, 10)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60c712-50c3-4ebe-94a9-eae8c1a1eebf",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: validation:auc and train:auc, and we elected to monitor validation:auc as you can see below. In this case, we only need to specify the metric name and do not need to provide regex. If you bring your own algorithm, your algorithm emits metrics by itself. In that case, you'll need to add a MetricDefinition object here to define the format of those metrics through regex, so that SageMaker knows how to extract those metrics from your CloudWatch logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdd85a0-ec0f-4835-9396-4deb57c46db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 19: Specify the objective metric (evaluation criterion) to be optimized\n",
    "objective_metric_name = 'validation:auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9d0f6-2336-421a-82e0-3cf733e3bf69",
   "metadata": {},
   "source": [
    "Now, we'll create a HyperparameterTuner object, to which we pass:\n",
    "- The XGBoost estimator we created above\n",
    "- Our hyperparameter ranges\n",
    "- Objective metric name and definition\n",
    "- Tuning resource configurations such as Number of training jobs to run in total and how many training jobs can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d54a99-2493-4213-9156-22b2caf961d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 20: Create a HyperparameterTuner object using all above-defined parameters\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=20,\n",
    "                            max_parallel_jobs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b9c52-996d-4424-ad13-dc492be07386",
   "metadata": {},
   "source": [
    "Now we can launch a hyperparameter tuning job by calling fit() function. After the hyperparameter tuning job is created, we can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83677526-9473-49fe-b388-3d8d9689848c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: xgboost-231116-1353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# cell 21: Launch a hyperparameter tuning job by calling fit() function [about 30' - check console]\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e389115-be84-4e3f-bddd-02870b987ebf",
   "metadata": {},
   "source": [
    "Let's just run a quick check of the hyperparameter tuning jobs status by using below command. Output should be “InProgress” . That means job started successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3afac11-693a-4ccd-bdb5-2f470fdc2912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Completed'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 22: check status of the hyperparameter tuning jobs\n",
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec3093-b878-4309-b072-3252aaee51fa",
   "metadata": {},
   "source": [
    "We can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed. It will take around 30 mins to gets completed (Training -> Hyperparameter tuning jobs).\n",
    "\n",
    "Once the tuning job is completed, you can pick the training job with the best performance, deploy, predict and evaluate the model developed by the job as done before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be8501aa-81ca-45dd-8546-2180a81ebbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-231116-1353-003-0661e71f'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 23: Return the best training job name\n",
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "434ce5c0-ef14-4e57-8f09-0a52b54ce9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-11-16 14:03:15 Starting - Preparing the instances for training\n",
      "2023-11-16 14:03:15 Downloading - Downloading input data\n",
      "2023-11-16 14:03:15 Training - Training image download completed. Training in progress.\n",
      "2023-11-16 14:03:15 Stopping - Stopping the training job\n",
      "2023-11-16 14:03:15 Uploading - Uploading generated training model\n",
      "2023-11-16 14:03:15 Stopped - Resource reused by training job: xgboost-231116-1353-009-23a73979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Job ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n",
      "INFO:sagemaker:Creating model with name: xgboost-2023-11-16-14-09-10-027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://sagemaker-eu-central-1-365644463685/sagemaker/DEMO-xgboost-dm/output/xgboost-231116-1353-003-0661e71f/output/model.tar.gz.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# cell 24: Deploy the best trained or user specified model to an Amazon SageMaker endpoint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tuner_predictor \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.m4.xlarge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/tuner.py:1354\u001b[0m, in \u001b[0;36mHyperparameterTuner.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, wait, model_name, kms_key, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m best_training_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_best_training_job()\n\u001b[1;32m   1352\u001b[0m best_estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator(best_training_job)\n\u001b[0;32m-> 1354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbest_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_instance_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbest_training_job\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1654\u001b[0m, in \u001b[0;36mEstimatorBase.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, serverless_inference_config, async_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m model\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m   1650\u001b[0m tags \u001b[38;5;241m=\u001b[39m update_inference_tags_with_jumpstart_training_tags(\n\u001b[1;32m   1651\u001b[0m     inference_tags\u001b[38;5;241m=\u001b[39mtags, training_tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m   1652\u001b[0m )\n\u001b[0;32m-> 1654\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_instance_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvolume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_recommendation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_recommendation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/model.py:1442\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_name, compiled_model_suffix))\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sagemaker_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m serverless_inference_config_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1450\u001b[0m     serverless_inference_config\u001b[38;5;241m.\u001b[39m_to_request_dict() \u001b[38;5;28;01mif\u001b[39;00m is_serverless \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m )\n\u001b[1;32m   1452\u001b[0m production_variant \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mproduction_variant(\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1454\u001b[0m     instance_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1460\u001b[0m     container_startup_health_check_timeout\u001b[38;5;241m=\u001b[39mcontainer_startup_health_check_timeout,\n\u001b[1;32m   1461\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/model.py:878\u001b[0m, in \u001b[0;36mModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m resolve_nested_dict_value_from_config(\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\n\u001b[1;32m    866\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    867\u001b[0m     MODEL_CONTAINERS_PATH,\n\u001b[1;32m    868\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session,\n\u001b[1;32m    869\u001b[0m )\n\u001b[1;32m    870\u001b[0m create_model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    871\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    872\u001b[0m     role\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    876\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    877\u001b[0m )\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcreate_model_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:3644\u001b[0m, in \u001b[0;36mSession.create_model\u001b[0;34m(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\u001b[0m\n\u001b[1;32m   3641\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3642\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 3644\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_model_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m name\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:5618\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5602\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5603\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5606\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5607\u001b[0m ):\n\u001b[1;32m   5608\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5609\u001b[0m \n\u001b[1;32m   5610\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5616\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:3632\u001b[0m, in \u001b[0;36mSession.create_model.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   3630\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreateModel request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   3631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3632\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3634\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://sagemaker-eu-central-1-365644463685/sagemaker/DEMO-xgboost-dm/output/xgboost-231116-1353-003-0661e71f/output/model.tar.gz."
     ]
    }
   ],
   "source": [
    "# cell 24: Deploy the best trained or user specified model to an Amazon SageMaker endpoint\n",
    "tuner_predictor = tuner.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba198834-12e5-4878-a38b-cee281d64856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 25: Create a serializer\n",
    "tuner_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71badcf2-5d9b-4de5-b69e-4ff843b6f14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 26 - Invoke inference endpoint with test set data for predictions\n",
    "predictions = predict(test_x.to_numpy(), tuner_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8683418-8440-4f46-b37b-34286fd398c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 27\n",
    "# Collect predictions and convert from the CSV output our model provides into a NumPy array\n",
    "pd.crosstab(index=test_y['y'].values, columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a78f8-ca47-400c-a532-8ad01486fbd7",
   "metadata": {},
   "source": [
    "### Clean-up\n",
    "\n",
    "If you are done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22270a-a74c-4cd5-b19b-3482d874451d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 28\n",
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628857f-618b-415e-8361-52677bc1fa5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 29\n",
    "tuner_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7f4ed-ead3-4351-968a-b523eeebc821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
